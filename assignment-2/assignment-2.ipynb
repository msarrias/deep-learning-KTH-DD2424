{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fun as f\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/user/Desktop/MSM/09-Deep Learning - KTH/assignments/assignment-2/Datasets/cifar-10-batches-py'\n",
    "result_pics = '/Users/user/Desktop/MSM/09-Deep Learning - KTH/assignments/assignment-1/Result_Pics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data:\n",
    "trainning_data = f.LoadBatch(os.path.join(path,'data_batch_1'))\n",
    "validation_data = f.LoadBatch(os.path.join(path,'data_batch_2'))\n",
    "test_data = f.LoadBatch(os.path.join(path,'test_batch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights and bias:\n",
    "W_1, b_1, W_2, b_2 = f.init_two_layers_w_b_param(trainning_data.data, trainning_data.labels, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute gradients analitically:\n",
    "grad_b_1, grad_b_2, grad_W_1, grad_W_2 = f.ComputeGradsAnalt(\n",
    "    trainning_data.data[:, 0:20], trainning_data.labels[:,0:20], b_1, b_2, W_1, W_2, lambda_=0)\n",
    "#Compute gradients numerically:\n",
    "#1.finite difference method.\n",
    "grad_b_1_n, grad_b_2_n, grad_W_1_n, grad_W_2_n = f.ComputeGradsNum(\n",
    "    trainning_data.data[:, 0:20], trainning_data.labels[:,0:20], W_1, W_2, b_1, b_2, h_=1e-6, lambda_=0)\n",
    "#2.centered difference method.\n",
    "grad_b_1_n_s, grad_b_2_n_s, grad_W_1_n_s, grad_W_2_n_s = f.ComputeGradsNumSlow(\n",
    "    trainning_data.data[:, 0:20], trainning_data.labels[:,0:20], W_1, W_2, b_1, b_2, h_=1e-6, lambda_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Relative error between gradients computed analitically and numerically using: finite diff method\n",
      "8.844392605193683e-06\n",
      "0.00010112146279395916\n",
      "0.003626248845054414\n",
      "6.050587200705587e-05\n",
      "Max Relative error between gradients computed analitically and numerically using: centered diff method\n",
      "8.546697380276077e-07\n",
      "1.1557400414400866e-07\n",
      "0.001961686602638457\n",
      "8.903622176761293e-06\n"
     ]
    }
   ],
   "source": [
    "print('Max Relative error between gradients computed analitically and numerically using: finite diff method')\n",
    "print(f.MaxRelativeError(grad_b_1, grad_b_1_n))\n",
    "print(f.MaxRelativeError(grad_b_2, grad_b_2_n))\n",
    "print(f.MaxRelativeError(grad_W_1, grad_W_1_n))\n",
    "print(f.MaxRelativeError(grad_W_2, grad_W_2_n))\n",
    "print('Max Relative error between gradients computed analitically and numerically using: centered diff method')\n",
    "print(f.MaxRelativeError(grad_b_1, grad_b_1_n_s))\n",
    "print(f.MaxRelativeError(grad_b_2, grad_b_2_n_s))\n",
    "print(f.MaxRelativeError(grad_W_1, grad_W_1_n_s))\n",
    "print(f.MaxRelativeError(grad_W_2, grad_W_2_n_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(X_, y_, W_1_, W_2_, b_1_, b_2_):\n",
    "    \"\"\"\n",
    "    :param X_:  N x d matrix,  trainning images.\n",
    "    :param y_:  N x 1 vector containing the trainning set true class.\n",
    "    :param W_: K x d matrix. Weights.\n",
    "    :param b_:  K x 1 vector. Bias.\n",
    "    :return acc: scalar.\n",
    "    \"\"\"\n",
    "    Npts = X_.shape[1]\n",
    "    ind_size_n = np.ones((Npts,1))\n",
    "    #S_1 = mxN matrix.\n",
    "    S_1 = np.matmul(W_1_, X_) + np.matmul(b_1_, ind_size_n.T)\n",
    "    #H = mxN matrix.\n",
    "    H = f.ReLU(S_1)\n",
    "    #S_2 = kxN matrix.\n",
    "    S_2 = np.matmul(W_2_, H) + np.matmul(b_2_, ind_size_n.T)\n",
    "    #P = kxN matrix.\n",
    "    P = f.SOFTMAX(S_2)\n",
    "    P_ = np.argmax(P,axis=0)\n",
    "    acc = np.count_nonzero(y_== P_) / float(len(P_))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_train = []\n",
    "cf_val = []\n",
    "cl_train = []\n",
    "cl_val = []\n",
    "acc_train = []\n",
    "acc_val = []\n",
    "\n",
    "GD_params = f.params(n_batch=10, eta=0.01, n_epochs=200)\n",
    "W_1, b_1, W_2, b_2 = f.init_two_layers_w_b_param(\n",
    "                                trainning_data.data[:, 0:100], trainning_data.labels[:,0:100], m_=50)\n",
    "\n",
    "for epoch in range(200):\n",
    "    for j in range(10):\n",
    "        j_start = j * GD_params.n_batch\n",
    "        j_end = (j + 1) * GD_params.n_batch\n",
    "        Xbatch = trainning_data.data[:, 0:100][:, j_start:j_end]\n",
    "        Ybatch = trainning_data.labels[:,0:100][:, j_start:j_end]\n",
    "\n",
    "#         Xbatch = trainning_data.data[:, 0:100]\n",
    "#         Ybatch = trainning_data.labels[:,0:100]\n",
    "\n",
    "        grad_b_1_t, grad_b_2_t, grad_W_1_t, grad_W_2_t = f.ComputeGradsAnalt(\n",
    "                                                    Xbatch, Ybatch, b_1, b_2, W_1, W_2, lambda_=0)\n",
    "\n",
    "        W_1 = W_1 - GD_params.eta * grad_W_1_t\n",
    "        b_1 = b_1 - GD_params.eta * grad_b_1_t\n",
    "        W_2 = W_2 - GD_params.eta * grad_W_2_t\n",
    "        b_2 = b_2 - GD_params.eta * grad_b_2_t\n",
    "\n",
    "    # cost per epoch\n",
    "\n",
    "    trainning_cost = f.ComputeCost(trainning_data.data, trainning_data.labels, W_1, W_2, b_1, b_2, lambda_=0)\n",
    "    validation_cost =f.ComputeCost(validation_data.data, validation_data.labels, W_1, W_2, b_1, b_2, lambda_=0)\n",
    "\n",
    "    # # loss per epoch\n",
    "    # trainning_loss = f.ComputeCost(X_, Y_.transpose(), W_, b_, 0)\n",
    "    # validation_loss = f.ComputeCost(X_val, Y_val.transpose(), W_, b_, 0)\n",
    "\n",
    "    cf_train.append(trainning_cost)\n",
    "    cf_val.append(validation_cost)\n",
    "\n",
    "    # cl_train.append(trainning_loss)\n",
    "    # cl_val.append(validation_loss)\n",
    "\n",
    "    # accuracy per epoch\n",
    "    acc_train_ = f.ComputeAccuracy(trainning_data.data, trainning_data.y, W_1, W_2, b_1, b_2)\n",
    "    acc_val_ = f.ComputeAccuracy(validation_data.data, validation_data.y, W_1, W_2, b_1, b_2)\n",
    "\n",
    "    acc_train.append(acc_train_)\n",
    "    acc_val.append(acc_val_)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, H = SOFTMAX(trainning_data.data, W_1, W_2, b_1, b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14036047e-02, 9.57492997e-03, 2.28200442e-04, ...,\n",
       "        2.32133844e-02, 1.01212594e-01, 9.51380435e-04],\n",
       "       [8.26587389e-03, 5.29646848e-03, 4.11120975e-04, ...,\n",
       "        1.69641158e-02, 1.54951397e-02, 3.98445840e-03],\n",
       "       [1.58021107e-03, 5.04170308e-03, 1.76807511e-04, ...,\n",
       "        2.89415439e-03, 6.71610121e-02, 1.08673847e-02],\n",
       "       ...,\n",
       "       [1.68895008e-04, 1.67246031e-04, 3.59973789e-04, ...,\n",
       "        7.65693320e-02, 9.30518032e-03, 7.70135595e-02],\n",
       "       [4.30344914e-05, 8.00113703e-04, 5.16034872e-03, ...,\n",
       "        1.79422939e-02, 3.48231603e-02, 9.86665920e-04],\n",
       "       [1.43827406e-04, 9.76424857e-01, 9.92172233e-01, ...,\n",
       "        5.13400625e-02, 6.30694424e-02, 5.75626014e-04]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainning_data.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.T trainning_data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(trainning_data.labels, p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(np.multiply(trainning_data.labels, p), axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOFTMAX(X_, W_1_, W_2_, b_1_, b_2_):\n",
    "    \"\"\" \n",
    "    :return p: kxN softmax\n",
    "    \"\"\"\n",
    "    ind_size_n = np.ones((X_.shape[1],1))\n",
    "    H_ = f.ReLU(X_, W_1_, b_1_)\n",
    "    S_2_ = np.matmul(W_2_, H_) + np.matmul(b_2_, ind_size_n.T)\n",
    "    p_ =  np.exp(S_2_) / np.matmul(np.ones((1, S_2_.shape[0])), np.exp(S_2_))\n",
    "    \n",
    "    #same as dividing by np.exp(s_)/ (np.sum(np.exp(s_), axis=0))\n",
    "    return p_ , H_\n",
    "\n",
    "def cross_entropy_loss(Y_, p_):\n",
    "    \"\"\"\n",
    "    :param Y_:\n",
    "    :param p_:\n",
    "    :return l_cross_:\n",
    "    \"\"\"\n",
    "    l_cross_ = -np.log(np.sum((np.multiply(Y_, p_)),axis=0))\n",
    "    return l_cross_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1404,\n",
       " 0.1597,\n",
       " 0.1692,\n",
       " 0.1763,\n",
       " 0.1805,\n",
       " 0.1839,\n",
       " 0.1865,\n",
       " 0.1879,\n",
       " 0.1911,\n",
       " 0.1919,\n",
       " 0.1921,\n",
       " 0.1937,\n",
       " 0.1941,\n",
       " 0.1953,\n",
       " 0.1951,\n",
       " 0.1963,\n",
       " 0.1961,\n",
       " 0.1972,\n",
       " 0.1964,\n",
       " 0.1971,\n",
       " 0.1976,\n",
       " 0.1969,\n",
       " 0.1972,\n",
       " 0.1984,\n",
       " 0.1989,\n",
       " 0.1988,\n",
       " 0.2003,\n",
       " 0.2015,\n",
       " 0.2028,\n",
       " 0.2028,\n",
       " 0.2027,\n",
       " 0.2032,\n",
       " 0.2041,\n",
       " 0.2047,\n",
       " 0.2048,\n",
       " 0.206,\n",
       " 0.2056,\n",
       " 0.2068,\n",
       " 0.2068,\n",
       " 0.207,\n",
       " 0.2077,\n",
       " 0.2079,\n",
       " 0.2083,\n",
       " 0.2091,\n",
       " 0.2099,\n",
       " 0.2098,\n",
       " 0.2108,\n",
       " 0.2095,\n",
       " 0.2098,\n",
       " 0.2107,\n",
       " 0.2111,\n",
       " 0.2114,\n",
       " 0.2112,\n",
       " 0.2109,\n",
       " 0.2112,\n",
       " 0.2119,\n",
       " 0.2116,\n",
       " 0.2109,\n",
       " 0.2105,\n",
       " 0.2096,\n",
       " 0.2102,\n",
       " 0.2101,\n",
       " 0.2104,\n",
       " 0.2099,\n",
       " 0.2092,\n",
       " 0.2099,\n",
       " 0.2096,\n",
       " 0.2101,\n",
       " 0.2102,\n",
       " 0.2098,\n",
       " 0.2094,\n",
       " 0.2098,\n",
       " 0.2095,\n",
       " 0.2094,\n",
       " 0.2097,\n",
       " 0.2096,\n",
       " 0.21,\n",
       " 0.2098,\n",
       " 0.2101,\n",
       " 0.2107,\n",
       " 0.2102,\n",
       " 0.2105,\n",
       " 0.2104,\n",
       " 0.2111,\n",
       " 0.211,\n",
       " 0.2106,\n",
       " 0.2107,\n",
       " 0.2108,\n",
       " 0.2111,\n",
       " 0.2112,\n",
       " 0.2117,\n",
       " 0.2118,\n",
       " 0.2122,\n",
       " 0.2122,\n",
       " 0.2125,\n",
       " 0.2121,\n",
       " 0.2119,\n",
       " 0.2118,\n",
       " 0.2117,\n",
       " 0.2118,\n",
       " 0.2118,\n",
       " 0.2119,\n",
       " 0.2119,\n",
       " 0.2123,\n",
       " 0.2127,\n",
       " 0.2127,\n",
       " 0.2132,\n",
       " 0.2132,\n",
       " 0.2134,\n",
       " 0.2135,\n",
       " 0.2136,\n",
       " 0.213,\n",
       " 0.2133,\n",
       " 0.2131,\n",
       " 0.2132,\n",
       " 0.2133,\n",
       " 0.2136,\n",
       " 0.2134,\n",
       " 0.2137,\n",
       " 0.2137,\n",
       " 0.2137,\n",
       " 0.2136,\n",
       " 0.2137,\n",
       " 0.2141,\n",
       " 0.2142,\n",
       " 0.2142,\n",
       " 0.2143,\n",
       " 0.2146,\n",
       " 0.2144,\n",
       " 0.2148,\n",
       " 0.2149,\n",
       " 0.2149,\n",
       " 0.2151,\n",
       " 0.2152,\n",
       " 0.215,\n",
       " 0.2151,\n",
       " 0.215,\n",
       " 0.2153,\n",
       " 0.2152,\n",
       " 0.2151,\n",
       " 0.2152,\n",
       " 0.2152,\n",
       " 0.2151,\n",
       " 0.2154,\n",
       " 0.2151,\n",
       " 0.2153,\n",
       " 0.2153,\n",
       " 0.2153,\n",
       " 0.2151,\n",
       " 0.215,\n",
       " 0.2149,\n",
       " 0.2149,\n",
       " 0.2151,\n",
       " 0.2149,\n",
       " 0.2148,\n",
       " 0.2149,\n",
       " 0.2146,\n",
       " 0.2145,\n",
       " 0.2146,\n",
       " 0.2145,\n",
       " 0.2145,\n",
       " 0.2144,\n",
       " 0.2144,\n",
       " 0.2143,\n",
       " 0.2145,\n",
       " 0.2143,\n",
       " 0.2143,\n",
       " 0.2144,\n",
       " 0.2142,\n",
       " 0.2142,\n",
       " 0.2143,\n",
       " 0.2143,\n",
       " 0.2144,\n",
       " 0.2142,\n",
       " 0.2139,\n",
       " 0.2141,\n",
       " 0.2139,\n",
       " 0.2141,\n",
       " 0.214,\n",
       " 0.2141,\n",
       " 0.2141,\n",
       " 0.2144,\n",
       " 0.2144,\n",
       " 0.2145,\n",
       " 0.2145,\n",
       " 0.2143,\n",
       " 0.2145,\n",
       " 0.2144,\n",
       " 0.2144,\n",
       " 0.2145,\n",
       " 0.2146,\n",
       " 0.2147,\n",
       " 0.2148,\n",
       " 0.2149,\n",
       " 0.2149,\n",
       " 0.215,\n",
       " 0.215,\n",
       " 0.215,\n",
       " 0.2151,\n",
       " 0.2151]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.295161446975457,\n",
       " 2.2798852019021614,\n",
       " 2.2700846499275076,\n",
       " 2.2626583119064914,\n",
       " 2.2575425333437322,\n",
       " 2.2540343919475134,\n",
       " 2.2514812968542035,\n",
       " 2.2499435493560824,\n",
       " 2.2482103357434617,\n",
       " 2.2493006251222183,\n",
       " 2.249626573534049,\n",
       " 2.250836901596127,\n",
       " 2.251723681198505,\n",
       " 2.254671001271081,\n",
       " 2.256802848911924,\n",
       " 2.261386917703268,\n",
       " 2.263888235365379,\n",
       " 2.268037101232046,\n",
       " 2.271504289423192,\n",
       " 2.2755133659767535,\n",
       " 2.2800290281738747,\n",
       " 2.28374729809842,\n",
       " 2.287949239404063,\n",
       " 2.293038231143196,\n",
       " 2.2965619769019505,\n",
       " 2.301065581607602,\n",
       " 2.3046971512980816,\n",
       " 2.3101142643942176,\n",
       " 2.313409760765639,\n",
       " 2.3185278650687735,\n",
       " 2.321832024536296,\n",
       " 2.326487597775877,\n",
       " 2.3318392901468292,\n",
       " 2.335997099582045,\n",
       " 2.340931293073432,\n",
       " 2.3464020167308774,\n",
       " 2.3522854150110337,\n",
       " 2.3568368809042717,\n",
       " 2.362912664378656,\n",
       " 2.36880068250207,\n",
       " 2.3746381025279306,\n",
       " 2.379504240484329,\n",
       " 2.385889754230395,\n",
       " 2.3920941357731573,\n",
       " 2.398306753205189,\n",
       " 2.4028394395016788,\n",
       " 2.4100216668366325,\n",
       " 2.4151273362465266,\n",
       " 2.4234309537876086,\n",
       " 2.427823348601094,\n",
       " 2.434240387246632,\n",
       " 2.4397919498515344,\n",
       " 2.446166502178483,\n",
       " 2.4518093852692653,\n",
       " 2.4592434482573213,\n",
       " 2.4657890362677337,\n",
       " 2.4717200043851837,\n",
       " 2.478130474606203,\n",
       " 2.4839385955851396,\n",
       " 2.4913154014444543,\n",
       " 2.4985482084194404,\n",
       " 2.5046377997075426,\n",
       " 2.511212465821686,\n",
       " 2.5179639630506303,\n",
       " 2.5246914737937027,\n",
       " 2.5318932417374316,\n",
       " 2.539149162751364,\n",
       " 2.545385397094162,\n",
       " 2.552428696948196,\n",
       " 2.559695482732052,\n",
       " 2.5656542260742676,\n",
       " 2.5735336506171946,\n",
       " 2.580707529892883,\n",
       " 2.586542891247074,\n",
       " 2.5940576439422567,\n",
       " 2.6003619797695166,\n",
       " 2.607250299694395,\n",
       " 2.613610685445245,\n",
       " 2.620999777094379,\n",
       " 2.627509659320349,\n",
       " 2.6340922227169403,\n",
       " 2.6405479711613116,\n",
       " 2.646593582994993,\n",
       " 2.6537432179265488,\n",
       " 2.6602362443274568,\n",
       " 2.666480726151465,\n",
       " 2.673028974959351,\n",
       " 2.679180981003051,\n",
       " 2.6856922103351653,\n",
       " 2.692318730723051,\n",
       " 2.6983478894486717,\n",
       " 2.7043808167187655,\n",
       " 2.7112995510137172,\n",
       " 2.7173279464620266,\n",
       " 2.7228765742558907,\n",
       " 2.729366439296062,\n",
       " 2.7352363210605937,\n",
       " 2.7413916137579353,\n",
       " 2.7471273805545877,\n",
       " 2.753926478326738,\n",
       " 2.7593344133809676,\n",
       " 2.765109275049911,\n",
       " 2.770491828426773,\n",
       " 2.7767753735779865,\n",
       " 2.781939582200436,\n",
       " 2.787943414113542,\n",
       " 2.793867482359328,\n",
       " 2.79915433695158,\n",
       " 2.804750153410413,\n",
       " 2.810231020593699,\n",
       " 2.815668277318149,\n",
       " 2.821245631250968,\n",
       " 2.826384542359051,\n",
       " 2.8318067372345914,\n",
       " 2.836870875606354,\n",
       " 2.8424786100372765,\n",
       " 2.8477722619631267,\n",
       " 2.8532950988611083,\n",
       " 2.8579563081799453,\n",
       " 2.8630018611177483,\n",
       " 2.8683845766073266,\n",
       " 2.873342979897004,\n",
       " 2.878645014435821,\n",
       " 2.8833985896388867,\n",
       " 2.8880352520506367,\n",
       " 2.8934266369430786,\n",
       " 2.898245991206156,\n",
       " 2.902821612938542,\n",
       " 2.907945281074404,\n",
       " 2.912485108034591,\n",
       " 2.9173828263691792,\n",
       " 2.9219127955588613,\n",
       " 2.926483703455938,\n",
       " 2.9311856150068163,\n",
       " 2.935535518766137,\n",
       " 2.940501588744163,\n",
       " 2.944678280250359,\n",
       " 2.9492644706992754,\n",
       " 2.9541039360453984,\n",
       " 2.9580003674684385,\n",
       " 2.9628444627027264,\n",
       " 2.967127335013154,\n",
       " 2.97100702862857,\n",
       " 2.975786555414532,\n",
       " 2.979749368797707,\n",
       " 2.983950104034603,\n",
       " 2.988375113368531,\n",
       " 2.992206122630687,\n",
       " 2.99649922054013,\n",
       " 3.0005838921222394,\n",
       " 3.0045201369993952,\n",
       " 3.0086545726755363,\n",
       " 3.0127572752161123,\n",
       " 3.0164617688709106,\n",
       " 3.0204545794512865,\n",
       " 3.0245330399633312,\n",
       " 3.0285530124639584,\n",
       " 3.03236521602452,\n",
       " 3.036032442858014,\n",
       " 3.040070932992648,\n",
       " 3.043949584892123,\n",
       " 3.0475398678221906,\n",
       " 3.0515422768302862,\n",
       " 3.0549749042351277,\n",
       " 3.058555186264847,\n",
       " 3.0628151624127495,\n",
       " 3.0660112214755086,\n",
       " 3.069905385553121,\n",
       " 3.073560452034828,\n",
       " 3.0768266309114782,\n",
       " 3.0806233572369406,\n",
       " 3.0841042886798866,\n",
       " 3.0877162169273578,\n",
       " 3.091003158216228,\n",
       " 3.094513998706214,\n",
       " 3.0979418391380977,\n",
       " 3.1018003850465465,\n",
       " 3.104867154983109,\n",
       " 3.1083920192591203,\n",
       " 3.111741429298913,\n",
       " 3.1148439371150327,\n",
       " 3.1183603082377815,\n",
       " 3.1213715734859973,\n",
       " 3.125046559459963,\n",
       " 3.1284395469349287,\n",
       " 3.1314794724021167,\n",
       " 3.1347869895457334,\n",
       " 3.137777920902164,\n",
       " 3.1414471098841763,\n",
       " 3.1442152975434103,\n",
       " 3.1476908580880423,\n",
       " 3.150586742818279,\n",
       " 3.153814893312537,\n",
       " 3.1570204240914874,\n",
       " 3.15997088650075,\n",
       " 3.1630109137348184,\n",
       " 3.166107829657646,\n",
       " 3.1690966038996184,\n",
       " 3.172138971979845,\n",
       " 3.1750784490832418]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_params = f.params(n_batch=10, eta=0.01, n_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchGD(X_, Y_, y_, X_val, Y_val, y_val, GD_params, lambda_=0):\n",
    "\n",
    "    Npts = X_.shape[0]\n",
    "    cf_train = []\n",
    "    cf_val = []\n",
    "    cl_train = []\n",
    "    cl_val = []\n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "\n",
    "    W_1, b_1, W_2, b_2 = f.init_two_layers_w_b_param(X_, Y_, m_=50)\n",
    "\n",
    "    for epoch in range(GD_params.n_epochs):\n",
    "        for j in range(Npts // GD_params.n_batch):\n",
    "            j_start = j * GD_params.n_batch\n",
    "            j_end = (j + 1) * GD_params.n_batch\n",
    "            Xbatch = X_[:, j_start:j_end]\n",
    "            Ybatch = Y_[:, j_start:j_end]\n",
    "\n",
    "            grad_b_1, grad_b_2, grad_w_1, grad_w_2 = f.ComputeGradsAnalt(\n",
    "                                                    Xbatch, Ybatch, b_1, b_2, W_1, W_2, lambda_=0)\n",
    "\n",
    "            W_1 = W_1 - GD_params.eta * grad_w_1\n",
    "            b_1 = b_1 - GD_params.eta * grad_b_1\n",
    "            W_2 = W_2 - GD_params.eta * grad_w_2\n",
    "            b_1 = b_2 - GD_params.eta * grad_b_2\n",
    "\n",
    "        # cost per epoch\n",
    "        trainning_cost = f.ComputeCost(X_, Y_, W_1, W_2, b_1, b_2, lambda_=0)\n",
    "        validation_cost = f.ComputeCost(X_val, Y_val, W_1, W_2, b_1, b_2, lambda_=0)\n",
    "\n",
    "        # loss per epoch\n",
    "        trainning_loss = f.ComputeCost(X_, Y_, W_1, W_2, b_1, b_2, 0)\n",
    "        validation_loss = f.ComputeCost(X_val, Y_val, W_1, W_2, b_1, b_2, 0)\n",
    "\n",
    "        cf_train.append(trainning_cost)\n",
    "        cf_val.append(validation_cost)\n",
    "\n",
    "        cl_train.append(trainning_loss)\n",
    "        cl_val.append(validation_loss)\n",
    "\n",
    "        # accuracy per epoch\n",
    "        acc_train_ = ComputeAccuracy(X_, y_, W_1, W_2, b_1, b_2)\n",
    "        acc_val_ = ComputeAccuracy(X_val, y_val, W_1, W_2, b_1, b_2)\n",
    "\n",
    "        acc_train.append(acc_train_)\n",
    "        acc_val.append(acc_val_)\n",
    "\n",
    "    return W_1, b_1, W_2, b_2, cf_train, cf_val, cl_train, cl_val, acc_train, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (50,10) (10,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ea0af2602beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mGD_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 lambda_=0)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-d1a38012aac5>\u001b[0m in \u001b[0;36mMiniBatchGD\u001b[0;34m(X_, Y_, y_, X_val, Y_val, y_val, GD_params, lambda_)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             grad_b_1, grad_b_2, grad_w_1, grad_w_2 = f.ComputeGradsAnalt(\n\u001b[0;32m---> 21\u001b[0;31m                                                     Xbatch, Ybatch, b_1, b_2, W_1, W_2, lambda_=0)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mW_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mGD_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_w_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MSM/09-Deep Learning - KTH/assignments/assignment-2/fun.py\u001b[0m in \u001b[0;36mComputeGradsAnalt\u001b[0;34m(X_, Y_, b_1_, b_2_, W_1_, W_2_, lambda_)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mComputeGradsAnalt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mNpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (50,10) (10,10) "
     ]
    }
   ],
   "source": [
    "W_1, b_1, W_2, b_2, cf_train, cf_val, cl_train, cl_val, acc_train, acc_val = MiniBatchGD(\n",
    "                trainning_data.data[:, 0:100],\n",
    "                trainning_data.labels[:, 0:100],\n",
    "                trainning_data.y[0:100],\n",
    "                validation_data.data[:, 0:100],\n",
    "                validation_data.labels[:, 0:100],\n",
    "                validation_data.y[0:100],\n",
    "                GD_params,\n",
    "                lambda_=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
