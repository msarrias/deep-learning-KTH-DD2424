\relax 
\providecommand\tcolorbox@label[2]{}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Maximum Relative error between numerical and analytical gradient vectors computations.\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:1}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=1$.\relax }}{1}}
\newlabel{fig:1}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=1$.\relax }}{2}}
\newlabel{fig:2}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=1$.\relax }}{2}}
\newlabel{fig:3}{{3}{2}}
