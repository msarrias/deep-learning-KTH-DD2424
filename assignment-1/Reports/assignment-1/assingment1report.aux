\relax 
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Exercise 1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\texttt  {lambda=0, n\_epochs=40, n\_batch=100, eta=.1}}{1}}
\newlabel{1.1}{{1.1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=0.1$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=0$.\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=0.1$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=0$.\relax }}{1}}
\newlabel{fig:1.1}{{2}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}\texttt  {lambda=0, n\_epochs=40, n\_batch=100, eta=.01}}{2}}
\newlabel{1.2}{{1.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=0$.\relax }}{2}}
\newlabel{fig:2}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=0$.\relax }}{2}}
\newlabel{fig:2.1}{{4}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}\texttt  {lambda=.1, n\_epochs=40, n\_batch=100, eta=.01}}{3}}
\newlabel{1.3}{{1.3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=0.1$.\relax }}{3}}
\newlabel{fig:3}{{5}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The learnt W matrix visualized as class template images. The network was trained with the following parameter settings: \texttt  {n\_batch}$=100$, \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=0.1$.\relax }}{3}}
\newlabel{fig:3.1}{{6}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}\texttt  {lambda=1, n\_epochs=40, n\_batch=100, eta=.01}}{4}}
\newlabel{1.4}{{1.4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The graph of the training and validation cost, loss and accuracy computed after every epoch. The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=100$ \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=1$.\relax }}{4}}
\newlabel{fig:4}{{7}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The learnt W matrix visualized as class template images. The network was trained with the following parameter settings: \texttt  {n\_batch}$=100$, \texttt  {eta}$=.01$, \texttt  {n\_epochs}$=40$ and \texttt  {lambda}$=1$.\relax }}{4}}
\newlabel{fig:4.1}{{8}{4}}
