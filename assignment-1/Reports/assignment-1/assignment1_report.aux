\relax 
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Exercise 2.1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Exercise 2.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=200$ \texttt  {eta}$=0.01$, \texttt  {n\_epochs}$=10$ and \texttt  {lambda}$=0.1$. Using the SVM multi-class loss function\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.1}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=200$ \texttt  {eta}$=0.01$, \texttt  {n\_epochs}$=10$ and \texttt  {lambda}$=0.1$ Using the cross-entropy loss.\relax }}{5}}
\newlabel{fig:1.2}{{2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=200$ \texttt  {eta}$=0.01$, \texttt  {n\_epochs}$=10$ and \texttt  {lambda}$=0.1$. Using the SVM multi-class loss function\relax }}{7}}
\newlabel{fig:1.1}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=200$ \texttt  {eta}$=0.01$, \texttt  {n\_epochs}$=10$ and \texttt  {lambda}$=0.1$ Using the cross-entropy loss.\relax }}{8}}
\newlabel{fig:1.6}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=200$ \texttt  {eta}$=0.0001$, \texttt  {n\_epochs}$=50$ and \texttt  {lambda}$=0.001$. Using the SVM multi-class loss function\relax }}{10}}
\newlabel{fig:1.1}{{5}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The network was trained with the following parameter settings$:$ \texttt  {n\_batch}$=200$ \texttt  {eta}$=0.0001$, \texttt  {n\_epochs}$=50$ and \texttt  {lambda}$=0.001$ Using the cross-entropy loss.\relax }}{11}}
\newlabel{fig:1.4}{{6}{11}}
