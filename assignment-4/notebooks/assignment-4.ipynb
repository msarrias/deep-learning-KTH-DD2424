{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x_vector, K, char_encoding):\n",
    "    X = np.zeros((K ,len(x_vector)))\n",
    "    x_encoding = [char_encoding[i] for i in x_vector]\n",
    "    for idx, x in enumerate(x_encoding):\n",
    "        X[x,idx]=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Vanilla Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN is a neural network which mainly works for processing sequential data. RNN models a dynamic system where the hidden state h at time t  depends on the input x at time t an also on the previous state h at time t-1. RNNs use hidden variables as a memory to capture long term information about a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Read in the data and get it ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_fname='data/goblet_book.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = open(book_fname, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chars=''.join(sorted(list(set(book_data))))\n",
    "K=len(book_chars) #len of all unique characters e.g\"' abcdefghijklmnopqrstuvwxyz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107542\n",
      "\t\n",
      " !\"'(),-./01234679:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ^_abcdefghijklmnopqrstuvwxyz}ü•\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(book_data))\n",
    "print(book_chars)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict key: letter, value letter idx.\n",
    "char_to_int = dict((letter, idx) for idx, letter in enumerate(book_chars))\n",
    "#dict key: letter idx, value letter.\n",
    "int_to_char = dict((idx, letter) for idx, letter in enumerate(book_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '\"': 4, \"'\": 5, '(': 6, ')': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '6': 17, '7': 18, '9': 19, ':': 20, ';': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'X': 46, 'Y': 47, 'Z': 48, '^': 49, '_': 50, 'a': 51, 'b': 52, 'c': 53, 'd': 54, 'e': 55, 'f': 56, 'g': 57, 'h': 58, 'i': 59, 'j': 60, 'k': 61, 'l': 62, 'm': 63, 'n': 64, 'o': 65, 'p': 66, 'q': 67, 'r': 68, 's': 69, 't': 70, 'u': 71, 'v': 72, 'w': 73, 'x': 74, 'y': 75, 'z': 76, '}': 77, 'ü': 78, '•': 79}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '\"', 5: \"'\", 6: '(', 7: ')', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '6', 18: '7', 19: '9', 20: ':', 21: ';', 22: '?', 23: 'A', 24: 'B', 25: 'C', 26: 'D', 27: 'E', 28: 'F', 29: 'G', 30: 'H', 31: 'I', 32: 'J', 33: 'K', 34: 'L', 35: 'M', 36: 'N', 37: 'O', 38: 'P', 39: 'Q', 40: 'R', 41: 'S', 42: 'T', 43: 'U', 44: 'V', 45: 'W', 46: 'X', 47: 'Y', 48: 'Z', 49: '^', 50: '_', 51: 'a', 52: 'b', 53: 'c', 54: 'd', 55: 'e', 56: 'f', 57: 'g', 58: 'h', 59: 'i', 60: 'j', 61: 'k', 62: 'l', 63: 'm', 64: 'n', 65: 'o', 66: 'p', 67: 'q', 68: 'r', 69: 's', 70: 't', 71: 'u', 72: 'v', 73: 'w', 74: 'x', 75: 'y', 76: 'z', 77: '}', 78: 'ü', 79: '•'}\n"
     ]
    }
   ],
   "source": [
    "print(int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Set hyper-parameters & initialize the RNN's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(book_data, m, eta, seq_length, sig):\n",
    "    data = {}\n",
    "    model_param= {}\n",
    "    data['book_data'] = book_data\n",
    "    data['K'] = len(''.join(sorted(list(set(data['book_data'])))))\n",
    "    model_param['m'] = m\n",
    "    model_param['eta'] = eta\n",
    "    model_param['seq_length'] = seq_length\n",
    "    model_param['sig'] = sig\n",
    "    h_0 = np.zeros((model_param['m'],1))\n",
    "    return data, model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model_param, data):\n",
    "    weights = {}\n",
    "    weights['V'] = np.random.normal(loc=0.0, scale=model_param['sig'],\n",
    "                                       size=( data['K'], model_param['m']))  #Kxm\n",
    "    weights['W'] = np.random.normal(loc=0.0, scale= model_param['sig'],\n",
    "                                       size=(model_param['m'], model_param['m'])) #mxm\n",
    "    weights['U'] = np.random.normal(loc=0.0, scale= model_param['sig'], \n",
    "                                       size=( model_param['m'], data['K'])) #mxK\n",
    "    weights['c'] = np.zeros((data['K'],1)) #Kx1\n",
    "    weights['b'] = np.zeros((model_param['m'],1)) #mx1 \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, model_param = init_param(book_data, m=5, eta=0.1, seq_length=25, sig=0.01)\n",
    "weights = init_weights(model_param, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARRY POTTER AND THE GOBL\n",
      "ARRY POTTER AND THE GOBLE\n"
     ]
    }
   ],
   "source": [
    "#The label for an input character is the next character of the text,\n",
    "#e.g: first input letter = H -> label : A\n",
    "#second input letter = A\n",
    "X_chars = book_data[0:model_param['seq_length']]\n",
    "print(X_chars)\n",
    "Y_chars = book_data[1:model_param['seq_length']+1]\n",
    "print(Y_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_encoding(X_chars, data['K'], char_to_int) #Kxseq_length\n",
    "Y = one_hot_encoding(Y_chars, data['K'], char_to_int) #Kxseq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoding = [char_to_int[i] for i in X_chars]\n",
    "y_encoding = [char_to_int[i] for i in Y_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True]\n"
     ]
    }
   ],
   "source": [
    "print(x_encoding == np.argmax(X, 0))\n",
    "print(y_encoding == np.argmax(Y, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 25) (80, 25)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.3 Synthesize text from your randomly initialized RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_char_sequence(x0, h_0, weights, n):\n",
    "    h, x = h_0, x0\n",
    "    xnext = np.zeros((x0.shape[0], n))\n",
    "    for i in range(n):\n",
    "        a =  np.dot(weights['W'], h) + np.dot(weights['U'], x) + weights['b'] #mx1\n",
    "        h = np.tanh(a) #mx1\n",
    "        o = np.dot(weights['V'], h) + weights['c'] #Kx1\n",
    "        p = softmax(o)\n",
    "        cp = np.cumsum(p) #k,\n",
    "        a = np.random.uniform(0, 1, 1)\n",
    "        ixs = np.where(cp - a > 0) #idx list\n",
    "        ii = ixs[0][0]\n",
    "        xnext[ii,i] = 1\n",
    "        x = xnext[:, i].reshape(-1, 1) #Kx1\n",
    "    return xnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros((data['K'],1)) #dummy input vector Kx1\n",
    "x0[char_to_int['.']] = 1\n",
    "h_0 = np.zeros((model_param['m'],1))\n",
    "xnext = synthesize_char_sequence(x0, h_0, weights, n=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 180)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "180.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "print(xnext.shape)\n",
    "sum(np.sum(xnext,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68, 24, 12, 27, 42, 44,  0, 22, 39,  7, 18, 19, 57, 50,  4, 32, 13,\n",
       "       32, 36, 14, 53, 69, 29, 60, 43, 53, 38, 31, 37, 32, 32, 15,  2, 40,\n",
       "       37,  6,  1, 40, 36, 66, 62, 24, 17,  0, 39, 71, 14, 46,  2, 76, 54,\n",
       "       31, 56, 30, 35,  5, 68, 69, 66, 51, 57, 63, 55, 73, 56, 75, 27,  7,\n",
       "       35, 65, 22, 47, 32, 28, 40, 64, 45, 15, 55, 33, 60, 63, 66, 28, 20,\n",
       "       60, 56, 62, 26, 40, 62, 24, 40, 50, 28, 29,  2, 46, 70, 77,  4, 56,\n",
       "       15, 15, 15, 59, 71,  1, 66, 37, 21, 38, 24, 61, 77, 59, 57, 34, 77,\n",
       "       44, 38, 30, 10,  9, 20, 24, 66, 36, 40, 11, 65, 73, 42, 67,  7, 47,\n",
       "       52, 22, 62, 79,  3, 12, 77, 41, 32, 57, 27, 14, 53, 48,  1, 37, 61,\n",
       "       66, 26, 26, 24, 57, 29, 62, 42, 52, 55, 23, 63, 34, 37, 46, 11, 25,\n",
       "       52, 22, 16, 30, 67, 78, 39, 52, 45, 75])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(xnext, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rB0ETV\t?Q)79g_\"J1JN2csGjUcPIOJJ3 RO(\n",
      "RNplB6\tQu2X zdIfHM'rspagmewfyE)Mo?YJFRnW3eKjmpF:jflDRlBR_FG Xt}\"f333iu\n",
      "pO;PBk}igL}VPH.-:BpNR/owTq)Yb?l•!0}SJgE2cZ\n",
      "OkpDDBgGlTbeAmLOX/Cb?4HqüQbWy\n"
     ]
    }
   ],
   "source": [
    "print(''.join(int_to_char[i] for i in np.argmax(xnext, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.4 Implement the forward & backward pass of back-prop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(weights, context_vect, X, Y):\n",
    "    ce_loss = 0\n",
    "    a_t, h_t, o_t, p_t = {}, {}, {}, {}\n",
    "    h_t[-1] = context_vect\n",
    "    n_T = X.shape[1]\n",
    "    for t in range(n_T):\n",
    "        a_t[t] = np.dot(weights['W'], h_t[t-1]) + np.dot(weights['U'], X[:,t].reshape(-1,1)) + weights['b'] #mx1\n",
    "        h_t[t] = np.tanh(a_t[t]) #mx1\n",
    "        o_t[t] = np.dot(weights['V'], h_t[t]) + weights['c'] #Kx1\n",
    "        p_t[t] = np.exp(o_t[t]) / np.matmul(np.ones((1, o_t[t].shape[0])), np.exp(o_t[t])) #Kx1\n",
    "        ce_loss += -np.log(np.dot(Y[:, t].reshape(1, -1), p_t[t]))\n",
    "    return h_t, p_t, ce_loss[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(weights, context_vect, X_, Y_):\n",
    "    #total number of states t\n",
    "    n_T = X_.shape[1]\n",
    "    #compute loss\n",
    "    h_t, p_t, ce_loss = compute_loss(weights, context_vect, X_, Y_)\n",
    "    \n",
    "    # backward pass\n",
    "    d_b, d_c = np.zeros_like(weights['b']), np.zeros_like(weights['c'])\n",
    "    d_ot, d_at, d_ht = {}, {}, {}\n",
    "    grads = {}\n",
    "    grads['V'] = 0\n",
    "    grads['W'] = 0\n",
    "    grads['U'] = 0\n",
    "    grads['c'] = np.zeros_like(weights['c']) #Kx1\n",
    "    grads['b'] = np.zeros_like(weights['b'])\n",
    "    \n",
    "    #as d_ht at time T = d_ot[t] * V, I initiallize d_a_next (d_at[t+1]) at zero,\n",
    "    #and later assign d_a_next  = d_at[t] \n",
    "    #this as d_ht at time t = d_ot[t] * V + d_at[t+1] * W \n",
    "    d_a_next = np.zeros((model_param['m'], 1))\n",
    "    \n",
    "    # we need to iterate backwards as for computing d_ht[t] we need d_at[t+1]\n",
    "    for t in reversed(range(n_T)):\n",
    "        d_ot[t] = -(Y_[:,t].reshape(-1, 1) - p_t[t]) #Kx1\n",
    "        grads['V'] += np.dot(d_ot[t], h_t[t].T) #Kxm\n",
    "        grads['c'] = grads['c'] + d_ot[t] #Kx1\n",
    "        d_ht[t] = np.dot(d_ot[t].T, weights['V']).T + np.dot(weights['W'].T, d_a_next) #mx1\n",
    "        d_at[t] = d_ht[t] * (1 - (h_t[t] * h_t[t])) #mx1\n",
    "        d_a_next = d_at[t]\n",
    "        grads['b'] =  grads['b'] + d_at[t] #mx1\n",
    "        grads['W'] += np.dot(d_at[t], h_t[t-1].T) #mxm\n",
    "        grads['U'] += np.dot(d_at[t], X_[:,t].reshape(1,-1)) #mxk\n",
    "    new_context_vector = h_t[n_T - 1].copy()\n",
    "    \n",
    "    # to avoid the exploding gradient\n",
    "    for keys, gradients in grads.items():\n",
    "        gradients = np.clip(gradients, -5, 5, out=gradients)\n",
    "    \n",
    "    return ce_loss[0], grads, new_context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumericalGradients(weights_,X_,Y_):\n",
    "    \"\"\"\n",
    "    Numerical gradients using Finite difference Formula.\n",
    "    \"\"\"\n",
    "    h_0 = np.zeros((model_param['m'],1))\n",
    "    h=1e-4\n",
    "    numerical_gradients = {}\n",
    "    for key, weight in weights_.items():\n",
    "        numerical_gradients[key] = np.zeros_like(weight)\n",
    "        #each weight(key) has a different dimension\n",
    "        for i in range(weights_[key].shape[0]):\n",
    "            for j in range(weights_[key].shape[1]):\n",
    "                # + h, I could have deep copied \n",
    "                weights_[key][i][j] += h\n",
    "                _, _, l2 = compute_loss(weights_, h_0, X_, Y_)\n",
    "                # as I have previously modified the weights(+h) I need to substract twice - h\n",
    "                weights_[key][i][j] -= 2*h\n",
    "                _, _, l1 = compute_loss(weights_, h_0, X_, Y_)\n",
    "                #initial weights\n",
    "                weights_[key][i][j] += h\n",
    "                numerical_gradients[key][i][j] = (l2-l1) / (2*h)\n",
    "    return numerical_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VerifyGradients(analytical_grad_dictionary, numerical_grad_dictionary):\n",
    "    max_relative_error = {}\n",
    "    eps = 1e-15\n",
    "    for (key_i,analit_weight), (key_j,num_weight) in zip(analytical_grad_dictionary.items(), numerical_grad_dictionary.items()):\n",
    "        max_value = np.maximum(eps, np.absolute(analit_weight) + np.absolute(num_weight))\n",
    "        max_relative_error[key_i] = np.amax(np.absolute(analit_weight-num_weight) / max_value)\n",
    "    return max_relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analytical gradients:\n",
    "loss, grads, _ = compute_gradients(weights, h_0, X, Y)\n",
    "#numerical gradients:\n",
    "numerical_gradients = NumericalGradients(weights,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V': 4.1144612911662657e-07,\n",
       " 'W': 1.4415513745363808e-06,\n",
       " 'U': 3.2111687931512057e-07,\n",
       " 'c': 1.1419375590515373e-09,\n",
       " 'b': 5.8311830205565415e-09}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VerifyGradients(grads,numerical_gradients )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.5 Train your RNN using AdaGrad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, model_param, weights, n_epochs = 10, update_step = 10000, n_=200, save_file = 'none'):\n",
    "    \n",
    "    book_data = data['book_data']\n",
    "    seq_length = model_param['seq_length']\n",
    "    K = data['K']\n",
    "    m = model_param['m']\n",
    "    eta = model_param['eta']\n",
    "    text_book_length = len(book_data)\n",
    "    iter_per_epoch= text_book_length // seq_length\n",
    "    total_num_iterations = n_epochs*iter_per_epoch\n",
    "    \n",
    "    m_weights = {}\n",
    "    for weight_key, weight_matrix in weights.items():\n",
    "        m_weights[weight_key] = np.zeros_like(weight_matrix)\n",
    "\n",
    "    train_loss = {}\n",
    "    smooth_loss = {}\n",
    "    best_model = {}\n",
    "    synth = {}\n",
    "    best_smooth_loss = 43\n",
    "    e = 0\n",
    "    h_0 = np.zeros((m, 1))\n",
    "    \n",
    "    print(f'epochs = {n_epochs}')\n",
    "    tic = time.time() \n",
    "    for iteration in range(total_num_iterations):\n",
    "\n",
    "        if iteration == 0 or e > (text_book_length - seq_length - 1):\n",
    "            h_0 = np.zeros((m, 1))\n",
    "            e = 0\n",
    "\n",
    "        #prepare data sequence\n",
    "        X_chars = book_data[e: e + seq_length]\n",
    "        Y_chars = book_data[e + 1: e + seq_length + 1]\n",
    "\n",
    "        X = one_hot_encoding(X_chars, K, char_to_int) #Kxseq_length\n",
    "        Y = one_hot_encoding(Y_chars, K, char_to_int) #Kxseq_length\n",
    "\n",
    "\n",
    "        #compute loss and grads\n",
    "        loss, grads, new_h_0 = compute_gradients(weights, h_0, X, Y)\n",
    "\n",
    "        smooth_loss_ = loss if e == 0 or iteration == 0 else 0.999 * smooth_loss_ + 0.001 * loss\n",
    "       \n",
    "        train_loss[iteration] = loss\n",
    "        smooth_loss[iteration] = smooth_loss_\n",
    "        \n",
    "        if smooth_loss_ < best_smooth_loss:\n",
    "                best_smooth_loss = smooth_loss_\n",
    "                best_model['loss'] = loss\n",
    "                best_model['smooth_loss'] = smooth_loss_\n",
    "                best_model['pred_text'] = xnext\n",
    "                best_model['grads'] = grads\n",
    "                best_model['weights'] = weights\n",
    "                best_model['h_0'] = h_0\n",
    "                \n",
    "        if (iteration - 1) % (update_step) == 0:\n",
    "            print('-------')\n",
    "            print(f'iteration = {iteration} / {total_num_iterations}')\n",
    "            print(f'Smooth loss = {float(smooth_loss_)}')\n",
    "            print('-------')\n",
    "            xnext = synthesize_char_sequence(X[:,[0]], h_0, weights, n=n_)\n",
    "            synth[iteration] = xnext\n",
    "            print(''.join(int_to_char[i] for i in np.argmax(xnext, axis=0)))\n",
    "\n",
    "        #apply adaGrad Algorithm\n",
    "        #It adapts the learning rate to the parameters, performing smaller updates\n",
    "        for key, weight in weights.items():\n",
    "            m_weights[key] += grads[key]**2\n",
    "            weights[key] = weights[key] -  eta  * grads[key] / np.sqrt(m_weights[key] + 1e-8)\n",
    "\n",
    "        e += seq_length\n",
    "\n",
    "        # new step context update\n",
    "        h_0 = new_h_0\n",
    "    tac = time.time() \n",
    "    print(f'training took {tac - tic} seconds')\n",
    "    \n",
    "    if save_file == 'none':\n",
    "        return train_loss, smooth_loss, best_model\n",
    "    \n",
    "    np.savez( save_file, train_loss=train_loss, smooth_loss=smooth_loss, best_model=best_model)\n",
    "    return train_loss, smooth_loss, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, model_param = init_param(book_data, m=100, eta=0.1, seq_length=25, sig=0.01)\n",
    "weights = init_weights(model_param, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 10\n",
      "-------\n",
      "iteration = 1 / 443010\n",
      "Smooth loss = 109.55385914941219\n",
      "-------\n",
      "1)OYtFjw\n",
      "))}VLE0GeWBum,DhTgBtoSYY,}?mvD\tLj^kp4(hXPQ1ü4_.pOeHGTLmGa,AMhTP•wHA.ANDz!eFLD?\"oi;p9XfLE4ü/29Qo.mo4,X\tw'/.UBEVkmS'LXFEE /L\t)T4sN(h'\tY;AkJ.A'7hbx)4y7\n",
      "B1kY9?9:A0\"yiD09R•.jj(;RFHW_rw0BIwW3l)cyVü\n",
      "-------\n",
      "iteration = 10001 / 443010\n",
      "Smooth loss = 52.60691545687797\n",
      "-------\n",
      "nours Ro her. dfoat.\"Wearlillor. Wayfien tha wharmid acoun, an, Wharkieg on of homas.  aikey eron theassear of a rus wor a chashillllerch the bow hilg  fom Harry . . . . Rmathis an istnle cot andy, an\n",
      "-------\n",
      "iteration = 20001 / 443010\n",
      "Smooth loss = 49.303036035218355\n",
      "-------\n",
      "the hire lowril.  HAr- said?\"\n",
      "\n",
      "Bar had led,\"\n",
      "Hagry wiseely tcoul allt.\n",
      "\"E'g.  Mad Petsing hid Parcor Hhorishert slakryra ar?\"\n",
      "\n",
      "\"Htimer for her dicker had set dely.\n",
      "\"Alling to kid mith dlefermanser?\" H\n",
      "-------\n",
      "iteration = 30001 / 443010\n",
      "Smooth loss = 47.94362252706097\n",
      "-------\n",
      "icking it taingy suareew pellodabing to goolvering the fiked that Rot.  To in morntin hereway had plap cindle gowl To vinter ag had Spoad.  Bnot. .T. vaim a tittor chea laigh wher; star putg.  beento \n",
      "-------\n",
      "iteration = 40001 / 443010\n",
      "Smooth loss = 46.76903222159519\n",
      "-------\n",
      "d nePlaking; flating ent,, seamen the waid; he didly the gold chest were hem sare a stalank not Qumped and Vording werpiog crame had manding thongaar squrst angelefting to the rang mangle.\n",
      "An agea hap\n",
      "-------\n",
      "iteration = 50001 / 443010\n",
      "Smooth loss = 47.80419399189599\n",
      "-------\n",
      "eppell all had wery, he dous, and in uthing the all a mashed, not mecly had saucle, Harring.  \"Weytcrough in they, the been unted on turnigga un hee ofest thing o.\"\n",
      "\"Theying be light  amone kindinn ou\n",
      "-------\n",
      "iteration = 60001 / 443010\n",
      "Smooth loss = 45.961093631280534\n",
      "-------\n",
      "Welssaing at the waice.  Itribuln the a sery, wenling Harry, binks explent.  On at wige fly wanking egghts sully.\n",
      "\" saud them they betinding.\n",
      "\"She gought it they turned was was grie him.\n",
      "\"I frontinger\n",
      "-------\n",
      "iteration = 70001 / 443010\n",
      "Smooth loss = 45.75219664143871\n",
      "-------\n",
      "igked to limcinged - with ball kibledait up brabe cagble Fad loding have Harry, stooke looked fill and quighe the saut.\n",
      "\"And foug, moow ghitk sougher, and awaling they weres hee dead anoow cout Andeam\n",
      "-------\n",
      "iteration = 80001 / 443010\n",
      "Smooth loss = 43.86967921359042\n",
      "-------\n",
      "ave straken, the from sle, fors do, attell was Aprecting reaped aurs Cluve mirting only enge, hidd and parcly he of thonsthen, pull dooksting the evipee!\"\n",
      "\"Werisat's about aroug, whong youn't prosks w\n",
      "-------\n",
      "iteration = 90001 / 443010\n",
      "Smooth loss = 60.92809525490248\n",
      "-------\n",
      " Warming he hissplought, ses sight won of filling rearsais, he cropt of his was shordeing Vornt, sich oul in had for then? fer Madands whisped the was poncure Dviacher sbuslinged the samsy of withe.  \n",
      "-------\n",
      "iteration = 100001 / 443010\n",
      "Smooth loss = 45.48344481741457\n",
      "-------\n",
      "ith eughcever.\n",
      "\"Weard him namsest it for rumbly.\n",
      "\"You vare hard them endifess you well, downave to or around that abre pot to afreggy fres year on a nouses grees werenr only to was who youd not you da\n",
      "-------\n",
      "iteration = 110001 / 443010\n",
      "Smooth loss = 43.31923584329762\n",
      "-------\n",
      "ars, whem.\"\n",
      "\"Nis thackich, the pithering injear Pettere have finged that the it ief a get?  We gackning head,\" said Harry towed in crouters into sled memie.  \"OWentoking. \"Ambeverever him, the it, loi\n",
      "-------\n",
      "iteration = 120001 / 443010\n",
      "Smooth loss = 43.69686368488\n",
      "-------\n",
      "ld lod inied was stalling mignt Profe werry, but was whatess - she toum ffon hgy wital and their want great?\" said Ron was Call its are.  Profesleass.  \"Did the Trim looked eyet the smilkwinds to beel\n",
      "-------\n",
      "iteration = 130001 / 443010\n",
      "Smooth loss = 42.384046089689626\n",
      "-------\n",
      " me haven, ese mank,\"\n",
      "He rearments coles doxes.  Sreak, \"Sibhthine to has has foie wite fortery In't's pes, his fimy.  \"Going up tar Sy sext.  \"Yeid on entcent sould wore foreeres to Verin's daskly he\n",
      "-------\n",
      "iteration = 140001 / 443010\n",
      "Smooth loss = 45.737590544883005\n",
      "-------\n",
      " thee the olly that gove Magich they ap to wiremd the gule himing up gless cont the Vernanded the Ron crave that's foath side into the acces.\n",
      "\"I hind wize, he gef to to deech saw te sluded, and go in \n",
      "-------\n",
      "iteration = 150001 / 443010\n",
      "Smooth loss = 42.67767810065654\n",
      "-------\n",
      "evers. him, boor go.  \"Fuccosis, he nonete's for threenfor Banies in and Hermionh for).\n",
      "\"Bid Frit tasiod.  There said coldesain.  Thilver tapovenken mmcayaned.  They no Mongh dode the bife oned it hal\n",
      "-------\n",
      "iteration = 160001 / 443010\n",
      "Smooth loss = 43.18712197345374\n",
      "-------\n",
      "g wish Cruly.  \"Wead.\n",
      " Kvermast how Thy tindarbening as too a shis . . . he whoir partleoke, the dolee dind and - pasped to us wizard marting were a youd and said?  Renthaiks feek that's who very jubh\n",
      "-------\n",
      "iteration = 170001 / 443010\n",
      "Smooth loss = 42.14216190470915\n",
      "-------\n",
      " shadd ground; the Dammy heads fac moss Sny hiskfies. . ... about thilk!\"  Malkive groom,\" ha so that ochely. Halred.  Neal tight.  He say gitly all onta Harry toed Danxile had sootion bucked Che's ba\n",
      "-------\n",
      "iteration = 180001 / 443010\n",
      "Smooth loss = 45.28091009596306\n",
      "-------\n",
      "d Harry the lood,\" saiding liggory alougd are a sibolf in outrendle.\n",
      "\"Me searly, he have wincing wask no, whith revidnactide, bad listle ter, schure!\"\n",
      "Voldes. ...\n",
      "Harry fingyor Frid.  Madre, Get his i\n",
      "-------\n",
      "iteration = 190001 / 443010\n",
      "Smooth loss = 43.403135725954094\n",
      "-------\n",
      "n tarm prust.\n",
      "NOther back aroutug like non will that in, E'plo!\" Dunging too, reved on his just and tos? The It the thoushed Hos for, Ron it lond itcher over in all sotick.. Trike you no my over thouc\n",
      "-------\n",
      "iteration = 200001 / 443010\n",
      "Smooth loss = 42.04454659973815\n",
      "-------\n",
      "and poblook nevered the Brianedly stees, there a fony Harry, ham oncol. . . would somest be off salkcowern't's and startouss, said was foo over be eisabs a wiftione wan by what of the adowwing tised, \n",
      "-------\n",
      "iteration = 210001 / 443010\n",
      "Smooth loss = 41.96730564821354\n",
      "-------\n",
      " now upparouse wanted were lough you und,\" said Harry, be the door,\" saround had just poly extled face a muphtousl to the too veesaan't ead her affroog oe the pucicurct. . . . the grinwing fet to math\n",
      "-------\n",
      "iteration = 220001 / 443010\n",
      "Smooth loss = 40.666663696627964\n",
      "-------\n",
      "he remest his was was dpone tew was upeney.  He were.  Azone you her mupbout gown, he haps; downtentaom, but it to on the ryou yerding, pered actee!\"  Freathe-bat Dumpledore encets shat wifken, the st\n",
      "-------\n",
      "iteration = 230001 / 443010\n",
      "Smooth loss = 41.348868002987\n",
      "-------\n",
      "  Harry at to the thind wan bulot?\"\n",
      "\"Fremss who why were Ron, the carkily of his eyes her!\"\n",
      "\"I're anxtonely.. Whishoted to meall, in in nough be line mark it they's taltlent, and hord?\"  sher had to a\n",
      "-------\n",
      "iteration = 240001 / 443010\n",
      "Smooth loss = 41.72742326160772\n",
      "-------\n",
      "g,\" Oit mashed into stelling lowl on hand.\n",
      "\"At it back wive tawn looked, sed misay.  Co in ontere Maxich.\n",
      "\"Ij a dabbed!\" said Harry,\" said Harry's gaclderning, plund an might a for, a woraty is to?\"\n",
      "\"\n",
      "-------\n",
      "iteration = 250001 / 443010\n",
      "Smooth loss = 42.44135011401356\n",
      "-------\n",
      " his paving thoughtors ognoves amse forently had antold.\n",
      "\"I've offliter, and soigh, sossionter Profes they his inst dreak.  Yout was the mechally Magical it bipe in inger that's scisitinss foore gowro\n",
      "-------\n",
      "iteration = 260001 / 443010\n",
      "Smooth loss = 40.65713906719304\n",
      "-------\n",
      "p, you known really.\n",
      "Harry the Harry on thign, to, Hermione notet,\" He waich no Harry sides, and this the stend, I. maured.  Sewing his robout unach up awrieders for of the amen paid, pott Harry curen\n",
      "-------\n",
      "iteration = 270001 / 443010\n",
      "Smooth loss = 42.27586470791947\n",
      "-------\n",
      ".. Aprione.\n",
      "\" Hermiored, beevels.  The gop wes good at then the wole had beformord the mone Potter elle.\n",
      "\"My of around Moody, sectiam had alley and their kereet durge at its evered you nove of took's \n",
      "-------\n",
      "iteration = 280001 / 443010\n",
      "Smooth loss = 42.5953485284276\n",
      "-------\n",
      "impes: Geilling got a refolty otes plicking inxtrouned it fmile. \"Come bang't normbayall. you coldess to and with ted couldy side, anstsurtly lought and not like of his Randing.  There.\"\n",
      "\"Itunr riddow\n",
      "-------\n",
      "iteration = 290001 / 443010\n",
      "Smooth loss = 40.96641040650545\n",
      "-------\n",
      "ns out in the omer belle a low happyons looked's hadben Fred my very thom the hask, \"U\n",
      "T\n",
      "\"Hage that thee?\"\n",
      "\"Gout Hunded hads with on Mo, whibe have shans. \"\n",
      "\"Thinked mechotsing acting every tonear. Sh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "iteration = 300001 / 443010\n",
      "Smooth loss = 39.60197392589072\n",
      "-------\n",
      "on'y it,\" said. Yeh, and could hands have grourderremiLf roundiesbant of Fide had 't'r half . ...\" He Side Hagrid boid in he long Dobbysing Harry.  \"He mensaed.\n",
      "\"You nonkeding soophto taps.\n",
      "\"When Croo\n",
      "-------\n",
      "iteration = 310001 / 443010\n",
      "Smooth loss = 39.72945001982676\n",
      "-------\n",
      " villing zingal come at than shor keen to ened that time you modd fret, Krusk'ur him eaticur.  \"Ar.  Co retid won't of oP Harry abaggread it,\" said Dumbledore's lult slad be osed I bin as to on the 2i\n",
      "-------\n",
      "iteration = 320001 / 443010\n",
      "Smooth loss = 40.989638140956956\n",
      "-------\n",
      "wsice uplarg was sore.\n",
      "Krigning..\n",
      "\"If into the thike, his never enosser,\" sarry move me elooking wanding them, liax the come Jang, permon't on the Broom, ecked the curmys.\n",
      "\"Yeah, \"On ragme-greet Krum \n",
      "-------\n",
      "iteration = 330001 / 443010\n",
      "Smooth loss = 40.785007607032945\n",
      "-------\n",
      "tap,\"  he dufforning feop e'choy.\n",
      "Neven sod of Mrs peen at to the was and again Cack robed when youn seft mittily Moody of the sworstunat? \n",
      "Doteheaned cangers Harry sedseats. Sarg \"around, will just a\n",
      "-------\n",
      "iteration = 340001 / 443010\n",
      "Smooth loss = 41.222782503124904\n",
      "-------\n",
      " down the upon an hissed.\n",
      "\"Thackhing Triared.  Dvandlater hat zavir get todoungbars at hed were gook und goigying for with Harry who were hand had to it's had been next took he wandle keet-wall muchte\n",
      "-------\n",
      "iteration = 350001 / 443010\n",
      "Smooth loss = 40.437785597831024\n",
      "-------\n",
      "u long, and any did looked his hear sifrance out sclased no just the Mining.  Hare sperfore and had smill mounged a wourds, and, he know to. but mup wee blacised serk abofking a to sayw, Ron. \"It thea\n",
      "-------\n",
      "iteration = 360001 / 443010\n",
      "Smooth loss = 42.16770799014469\n",
      "-------\n",
      "ir one him!\"\n",
      "Oor... him, ealle contarencalf when abowt was fold, Ron of an ole los on st Mr. Crount, do boWed be shook-ere's as minay dowly amoward,\" said Mr. Weasleys' me lagky, into see sheet didn't\n",
      "-------\n",
      "iteration = 370001 / 443010\n",
      "Smooth loss = 40.36884991519158\n",
      "-------\n",
      " saidy\n",
      "\"Peawing al. They rifes both he lut the shoun.  There looked and seen, Fred belaksing cured congy to offtenged in frairs.  Madge see yourtly angrery to out alarg Harry secting to smmoider at're\n",
      "-------\n",
      "iteration = 380001 / 443010\n",
      "Smooth loss = 40.88367755957947\n",
      "-------\n",
      "thee,\" Hermione brabbight; for at on their cwaied know dlathiy monframars\" he shadring scarmelin's Suons with exind's voice-coutkn't him Poot they.  \"I'm frogtrouching to instonsces a made at ending t\n",
      "-------\n",
      "iteration = 390001 / 443010\n",
      "Smooth loss = 39.306155400841405\n",
      "-------\n",
      "e the Twortly -bother quicked?\"\n",
      "\"Oh kicked Ron laar Maviding mivanip Vour.  \"Neventered you, sirnor of worldins  He bates paredly Murgrertur,.\n",
      "\"I werved at it up the vould cersion.  \"Ludh of the summa\n",
      "-------\n",
      "iteration = 400001 / 443010\n",
      "Smooth loss = 45.22833345001041\n",
      "-------\n",
      "e wournentle.  Eull had trating the wnack for loulder his fors aroug, fallle thead tkunk; wes, an of civing to the roilly bup out his swespounnam plass, in, biy Harly and and who was wastrart. .\" sorr\n",
      "-------\n",
      "iteration = 410001 / 443010\n",
      "Smooth loss = 41.830329222066766\n",
      "-------\n",
      "you hand over rand.  Snape was speainding really ey of yet wizards boticius aning horss\"\n",
      "\"You of exceveos plictying to but fingers as mool on stiling in they warg stshinot this to tine - they, \"The cl\n",
      "-------\n",
      "iteration = 420001 / 443010\n",
      "Smooth loss = 39.781225997007\n",
      "-------\n",
      "it exching.  It are them wand!   Helking cannated thought of they untinble loyled. A Krubsts zoomedrightes ingelt gown.\n",
      "Harry 'ight buck \n",
      "Tubblewin, and Fudd him to was when one uny some thee that hap\n",
      "-------\n",
      "iteration = 430001 / 443010\n",
      "Smooth loss = 40.66237174549986\n",
      "-------\n",
      "bit like trapped an eached the very conges Potter suirthe!\" Ron hourliageing yet.\n",
      "\"Set yel.  bodror, Draco,\"\n",
      "\"I not away.  In to make teot ale that gget.  Slapw every.\n",
      "\"BuTnelly feetalte bowly (math a\n",
      "-------\n",
      "iteration = 440001 / 443010\n",
      "Smooth loss = 39.0610198968989\n",
      "-------\n",
      "e land lowd the juine tol Potter, sire trother had parely damma petting perk Mad-Ey sprcking, theelteds as you.  He?., and hat duMut he could finters turned lerved of the farery you jumy.  \"Down as lu\n",
      "training took 1218.229176044464 seconds\n"
     ]
    }
   ],
   "source": [
    "train_loss, smooth_loss, best_model= train_model(data, model_param, weights, \n",
    "                                                n_epochs = 10, update_step = 10000, n_=200, \n",
    "                                                save_file = 'results_180719')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.30192759989202"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model['smooth_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3))\n",
    "ax1.plot(smooth_loss.keys(), list(smooth_loss.values()), color='tab:blue')\n",
    "ax1.set_title('(a)')\n",
    "ax1.set_ylabel('Smooth loss')\n",
    "ax1.set_xlabel('update step')\n",
    "ax2.plot([i for i in range(1,len(smooth_loss.keys()),10000)], \n",
    "         [smooth_list[i] for i in [i for i in range(1,len(smooth_loss.keys()),10000)]],\n",
    "         color='tab:blue')\n",
    "ax2.set_title('(b)')\n",
    "ax2.set_ylabel('Smooth loss')\n",
    "ax2.set_xlabel('update step')\n",
    "plt.savefig('test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arne greep.  It and Snapplimple Vobled to Harry exatp and sud how Weasley swenter.\n",
      "\"Well, but have chilly down awainuy to his smyong the Ro.  Ansts-out Fudd the polwardly,\" seersing to the fectort at asmeres and me brenss asseng over -  no!\"\n",
      "\"Mock.\n",
      "\"Com over us only not almost, my the nears, gobgley, but when the momens the. . .\n",
      "\"Beat, I'll below, his when yet half feet was cooked enougn your.  Whicking back.  He nelling; is don't, him.\"\n",
      "Fudd cood, way minateen still pleet to where, what,\" said Moody orking his nis?, Fred loiden.\n",
      "Harry turned him.  Fther.  Mrs. Weasley would names. No.  bas, went thenesler he Num!  Hordt flat and suir.\n",
      "Disstusenge pleas.  \"Hogkers din on the Geove had hest of didfer, allod.  AGon ort to clos to heape.\n",
      "\" you happened,\"\"Ghe someysosped where, sal sudnermally were levere.  Yo Ron be dot they of Lure thedely, vidding one of a mistering of sours looked its.\n",
      "\"Why and Fred thit's would trien they went.\n",
      "He?\"\n",
      "Bo non, I.\"R Harry's, that Hogly voissly.  bas talki\n"
     ]
    }
   ],
   "source": [
    "xnext = synthesize_char_sequence(X[:,[0]], best_model['h_0'], best_model['weights'], n=1000)\n",
    "print(''.join(int_to_char[i] for i in np.argmax(xnext, axis=0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
